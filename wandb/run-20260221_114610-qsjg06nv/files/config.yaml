_wandb:
    value:
        cli_version: 0.22.2
        e:
            2evadpq0oj2venm622q4whhjs6encs1u:
                args:
                    - --model_config
                    - configs/llama_350m.json
                    - --lr
                    - "0.001"
                    - --galore_scale
                    - "0.25"
                    - --rank
                    - "256"
                    - --update_proj_gap
                    - "200"
                    - --batch_size
                    - "128"
                    - --total_batch_size
                    - "512"
                    - --num_training_steps
                    - "60000"
                    - --warmup_steps
                    - "6000"
                    - --weight_decay
                    - "0"
                    - --dtype
                    - bfloat16
                    - --eval_every
                    - "1000"
                    - --optimizer
                    - galore_adamw
                    - --save_every
                    - "20000"
                    - --save_dir
                    - /data/checkpoint/OSD_350M
                    - --seed
                    - "777"
                    - --name
                    - OSD_350M
                    - --proj_type
                    - continuous
                    - --grad_clipping
                    - "1.0"
                codePath: torchrun_main.py
                codePathLocal: torchrun_main.py
                cpu_count: 64
                cpu_count_logical: 128
                cudaVersion: "12.8"
                disk:
                    /:
                        total: "3749013602304"
                        used: "2993277497344"
                email: keunyoung.kim@snu.ac.kr
                executable: /opt/conda/envs/galore/bin/python
                git:
                    remote: git@github.com:kykim23/Online-Subspace-Descent.git
                gpu: NVIDIA RTX A6000
                gpu_count: 8
                gpu_nvidia:
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-81b93137-48e6-4658-66b6-4e939601f0c1
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-12705f3b-859f-fb2d-41a7-e0967e05be51
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-06a1b3ce-1c7e-fc76-ab83-d9fb63ec1653
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-158ab6a7-3394-0eec-29a0-640939b4fd22
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-1914af91-ab45-0f3f-2981-ef84c0f771e6
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-fd25e90c-13b6-4014-d68a-149710676995
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-0a66b9b7-ccf9-04ab-7a7d-9b8b7c1b1b82
                    - architecture: Ampere
                      cudaCores: 10752
                      memoryTotal: "51527024640"
                      name: NVIDIA RTX A6000
                      uuid: GPU-fda21acb-5c4a-b096-bc67-9ae0834f24ba
                host: f9abbe3f582f
                memory:
                    total: "270076493824"
                os: Linux-5.4.0-216-generic-x86_64-with-glibc2.39
                program: /workspace/Online-Subspace-Descent/torchrun_main.py
                python: CPython 3.11.13
                root: /workspace/Online-Subspace-Descent
                startedAt: "2026-02-21T11:46:10.413065Z"
                writerId: 2evadpq0oj2venm622q4whhjs6encs1u
        m: []
        python_version: 3.11.13
        t:
            "1":
                - 1
                - 11
                - 49
                - 51
                - 71
            "2":
                - 1
                - 11
                - 49
                - 51
                - 71
            "3":
                - 3
                - 61
            "4": 3.11.13
            "5": 0.22.2
            "6": 4.31.0
            "12": 0.22.2
            "13": linux-x86_64
activation_checkpointing:
    value: false
batch_size:
    value: 128
beta1:
    value: 0
continue_from:
    value: null
dataset:
    value: c4
device:
    value: cuda:0
dtype:
    value: bfloat16
eval_every:
    value: 1000
galore_scale:
    value: 0.25
grad_clipping:
    value: 1
gradient_accumulation:
    value: 1
log_details:
    value: false
max_length:
    value: 256
max_lr:
    value: 0.001
max_train_tokens:
    value: null
min_lr_ratio:
    value: 0.1
model:
    value:
        _name_or_path: configs/llama_350m.json
        add_cross_attention: false
        architectures:
            - LLaMAForCausalLM
        bad_words_ids: null
        begin_suppress_tokens: null
        bos_token_id: 0
        chunk_size_feed_forward: 0
        cross_attention_hidden_size: null
        decoder_start_token_id: null
        diversity_penalty: 0
        do_sample: false
        early_stopping: false
        encoder_no_repeat_ngram_size: 0
        eos_token_id: 1
        exponential_decay_length_penalty: null
        finetuning_task: null
        forced_bos_token_id: null
        forced_eos_token_id: null
        hidden_act: silu
        hidden_size: 1024
        id2label:
            "0": LABEL_0
            "1": LABEL_1
        initializer_range: 0.02
        intermediate_size: 2736
        is_decoder: false
        is_encoder_decoder: false
        label2id:
            LABEL_0: 0
            LABEL_1: 1
        length_penalty: 1
        max_length: 20
        max_position_embeddings: 2048
        max_sequence_length: 1024
        min_length: 0
        model_type: llama
        no_repeat_ngram_size: 0
        num_attention_heads: 16
        num_beam_groups: 1
        num_beams: 1
        num_hidden_layers: 24
        num_key_value_heads: 16
        num_return_sequences: 1
        output_attentions: false
        output_hidden_states: false
        output_scores: false
        pad_token_id: -1
        prefix: null
        pretraining_tp: 1
        problem_type: null
        remove_invalid_values: false
        repetition_penalty: 1
        return_dict: true
        return_dict_in_generate: false
        rms_norm_eps: 1e-06
        rope_scaling: null
        sep_token_id: null
        suppress_tokens: null
        task_specific_params: null
        temperature: 1
        tf_legacy_loss: false
        tie_encoder_decoder: false
        tie_word_embeddings: false
        tokenizer_class: null
        top_k: 50
        top_p: 1
        torch_dtype: null
        torchscript: false
        transformers_version: 4.31.0
        typical_p: 1
        use_bfloat16: false
        use_cache: true
        vocab_size: 32000
model_config:
    value: configs/llama_350m.json
name:
    value: OSD_350M
num_training_steps:
    value: 60000
optimizer:
    value: galore_adamw
proj_type:
    value: continuous
rank:
    value: 256
save_dir:
    value: /data/checkpoint/OSD_350M
save_every:
    value: 20000
scheduler:
    value: cosine
seed:
    value: 777
single_gpu:
    value: false
tags:
    value: null
total_batch_size:
    value: 512
total_params_M:
    value: 367.96928
update_proj_gap:
    value: 200
use_hf_model:
    value: false
use_torch_compile:
    value: false
warmup_steps:
    value: 6000
weight_decay:
    value: 0
workers:
    value: 8
world_size:
    value: 4
