[32m2026-02-21 11:46:14.478[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m166[0m - [1mUsing dist with rank 0 (only rank 0 will log)[0m
[32m2026-02-21 11:46:14.479[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m167[0m - [1m****************************************[0m
[32m2026-02-21 11:46:14.479[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m168[0m - [1mStarting training with the arguments[0m
[32m2026-02-21 11:46:14.479[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mmodel_config                   configs/llama_350m.json[0m
[32m2026-02-21 11:46:14.480[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1muse_hf_model                   False[0m
[32m2026-02-21 11:46:14.480[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mcontinue_from                  None[0m
[32m2026-02-21 11:46:14.480[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mbatch_size                     128[0m
[32m2026-02-21 11:46:14.480[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mgradient_accumulation          1[0m
[32m2026-02-21 11:46:14.480[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mtotal_batch_size               512[0m
[32m2026-02-21 11:46:14.481[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mmax_length                     256[0m
[32m2026-02-21 11:46:14.481[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1moptimizer                      galore_adamw[0m
[32m2026-02-21 11:46:14.481[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mlr                             0.001[0m
[32m2026-02-21 11:46:14.481[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mscheduler                      cosine[0m
[32m2026-02-21 11:46:14.481[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mmin_lr_ratio                   0.1[0m
[32m2026-02-21 11:46:14.482[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mactivation_checkpointing       False[0m
[32m2026-02-21 11:46:14.482[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mweight_decay                   0.0[0m
[32m2026-02-21 11:46:14.482[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mwarmup_steps                   6000[0m
[32m2026-02-21 11:46:14.482[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1meval_every                     1000[0m
[32m2026-02-21 11:46:14.482[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mnum_training_steps             60000[0m
[32m2026-02-21 11:46:14.483[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mmax_train_tokens               None[0m
[32m2026-02-21 11:46:14.483[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1msave_every                     20000[0m
[32m2026-02-21 11:46:14.483[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1msave_dir                       /data/checkpoint/OSD_350M[0m
[32m2026-02-21 11:46:14.483[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mtags                           None[0m
[32m2026-02-21 11:46:14.483[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mdtype                          bfloat16[0m
[32m2026-02-21 11:46:14.484[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1muse_torch_compile              False[0m
[32m2026-02-21 11:46:14.484[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mworkers                        8[0m
[32m2026-02-21 11:46:14.484[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mseed                           777[0m
[32m2026-02-21 11:46:14.484[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mname                           OSD_350M[0m
[32m2026-02-21 11:46:14.484[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mgrad_clipping                  1.0[0m
[32m2026-02-21 11:46:14.485[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mbeta1                          0.0[0m
[32m2026-02-21 11:46:14.485[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mrank                           256[0m
[32m2026-02-21 11:46:14.485[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mupdate_proj_gap                200[0m
[32m2026-02-21 11:46:14.485[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mgalore_scale                   0.25[0m
[32m2026-02-21 11:46:14.486[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mproj_type                      continuous[0m
[32m2026-02-21 11:46:14.486[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1mlog_details                    False[0m
[32m2026-02-21 11:46:14.486[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m170[0m - [1msingle_gpu                     False[0m
[32m2026-02-21 11:46:14.486[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m171[0m - [1m****************************************[0m
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:00<00:00, 23839.61it/s]
Resolving data files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1024/1024 [00:00<00:00, 24443.92it/s]
[32m2026-02-21 11:46:21.540[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m177[0m - [1mShuffling data with seed 777[0m
/opt/conda/envs/galore/lib/python3.11/site-packages/huggingface_hub/file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[34m[1mwandb[0m: [33mWARNING[0m Saving files without folders. If you want to preserve subdirectories pass base_path to wandb.save, i.e. wandb.save("/mnt/folder/file.h5", base_path="/mnt")
Update steps:   0%|                                   | 0/60000 [00:00<?, ?it/s]
enable GaLore for weights in module:  model.layers.0.self_attn.q_proj
enable GaLore for weights in module:  model.layers.0.self_attn.k_proj
enable GaLore for weights in module:  model.layers.0.self_attn.v_proj
enable GaLore for weights in module:  model.layers.0.self_attn.o_proj
enable GaLore for weights in module:  model.layers.0.mlp.gate_proj
enable GaLore for weights in module:  model.layers.0.mlp.down_proj
enable GaLore for weights in module:  model.layers.0.mlp.up_proj
enable GaLore for weights in module:  model.layers.1.self_attn.q_proj
enable GaLore for weights in module:  model.layers.1.self_attn.k_proj
enable GaLore for weights in module:  model.layers.1.self_attn.v_proj
enable GaLore for weights in module:  model.layers.1.self_attn.o_proj
enable GaLore for weights in module:  model.layers.1.mlp.gate_proj
enable GaLore for weights in module:  model.layers.1.mlp.down_proj
enable GaLore for weights in module:  model.layers.1.mlp.up_proj
enable GaLore for weights in module:  model.layers.2.self_attn.q_proj
enable GaLore for weights in module:  model.layers.2.self_attn.k_proj
enable GaLore for weights in module:  model.layers.2.self_attn.v_proj
enable GaLore for weights in module:  model.layers.2.self_attn.o_proj
enable GaLore for weights in module:  model.layers.2.mlp.gate_proj
enable GaLore for weights in module:  model.layers.2.mlp.down_proj
enable GaLore for weights in module:  model.layers.2.mlp.up_proj
enable GaLore for weights in module:  model.layers.3.self_attn.q_proj
enable GaLore for weights in module:  model.layers.3.self_attn.k_proj
enable GaLore for weights in module:  model.layers.3.self_attn.v_proj
enable GaLore for weights in module:  model.layers.3.self_attn.o_proj
enable GaLore for weights in module:  model.layers.3.mlp.gate_proj
enable GaLore for weights in module:  model.layers.3.mlp.down_proj
enable GaLore for weights in module:  model.layers.3.mlp.up_proj
enable GaLore for weights in module:  model.layers.4.self_attn.q_proj
enable GaLore for weights in module:  model.layers.4.self_attn.k_proj
enable GaLore for weights in module:  model.layers.4.self_attn.v_proj
enable GaLore for weights in module:  model.layers.4.self_attn.o_proj
enable GaLore for weights in module:  model.layers.4.mlp.gate_proj
enable GaLore for weights in module:  model.layers.4.mlp.down_proj
enable GaLore for weights in module:  model.layers.4.mlp.up_proj
enable GaLore for weights in module:  model.layers.5.self_attn.q_proj
enable GaLore for weights in module:  model.layers.5.self_attn.k_proj
enable GaLore for weights in module:  model.layers.5.self_attn.v_proj
enable GaLore for weights in module:  model.layers.5.self_attn.o_proj
enable GaLore for weights in module:  model.layers.5.mlp.gate_proj
enable GaLore for weights in module:  model.layers.5.mlp.down_proj
enable GaLore for weights in module:  model.layers.5.mlp.up_proj
enable GaLore for weights in module:  model.layers.6.self_attn.q_proj
enable GaLore for weights in module:  model.layers.6.self_attn.k_proj
enable GaLore for weights in module:  model.layers.6.self_attn.v_proj
enable GaLore for weights in module:  model.layers.6.self_attn.o_proj
enable GaLore for weights in module:  model.layers.6.mlp.gate_proj
enable GaLore for weights in module:  model.layers.6.mlp.down_proj
enable GaLore for weights in module:  model.layers.6.mlp.up_proj
enable GaLore for weights in module:  model.layers.7.self_attn.q_proj
enable GaLore for weights in module:  model.layers.7.self_attn.k_proj
enable GaLore for weights in module:  model.layers.7.self_attn.v_proj
enable GaLore for weights in module:  model.layers.7.self_attn.o_proj
enable GaLore for weights in module:  model.layers.7.mlp.gate_proj
enable GaLore for weights in module:  model.layers.7.mlp.down_proj
enable GaLore for weights in module:  model.layers.7.mlp.up_proj
enable GaLore for weights in module:  model.layers.8.self_attn.q_proj
enable GaLore for weights in module:  model.layers.8.self_attn.k_proj
enable GaLore for weights in module:  model.layers.8.self_attn.v_proj
enable GaLore for weights in module:  model.layers.8.self_attn.o_proj
enable GaLore for weights in module:  model.layers.8.mlp.gate_proj
enable GaLore for weights in module:  model.layers.8.mlp.down_proj
enable GaLore for weights in module:  model.layers.8.mlp.up_proj
enable GaLore for weights in module:  model.layers.9.self_attn.q_proj
enable GaLore for weights in module:  model.layers.9.self_attn.k_proj
enable GaLore for weights in module:  model.layers.9.self_attn.v_proj
enable GaLore for weights in module:  model.layers.9.self_attn.o_proj
enable GaLore for weights in module:  model.layers.9.mlp.gate_proj
enable GaLore for weights in module:  model.layers.9.mlp.down_proj
enable GaLore for weights in module:  model.layers.9.mlp.up_proj
enable GaLore for weights in module:  model.layers.10.self_attn.q_proj
enable GaLore for weights in module:  model.layers.10.self_attn.k_proj
enable GaLore for weights in module:  model.layers.10.self_attn.v_proj
enable GaLore for weights in module:  model.layers.10.self_attn.o_proj
enable GaLore for weights in module:  model.layers.10.mlp.gate_proj
enable GaLore for weights in module:  model.layers.10.mlp.down_proj
enable GaLore for weights in module:  model.layers.10.mlp.up_proj
enable GaLore for weights in module:  model.layers.11.self_attn.q_proj
enable GaLore for weights in module:  model.layers.11.self_attn.k_proj
enable GaLore for weights in module:  model.layers.11.self_attn.v_proj
enable GaLore for weights in module:  model.layers.11.self_attn.o_proj
enable GaLore for weights in module:  model.layers.11.mlp.gate_proj
enable GaLore for weights in module:  model.layers.11.mlp.down_proj
enable GaLore for weights in module:  model.layers.11.mlp.up_proj
enable GaLore for weights in module:  model.layers.12.self_attn.q_proj
enable GaLore for weights in module:  model.layers.12.self_attn.k_proj
enable GaLore for weights in module:  model.layers.12.self_attn.v_proj
enable GaLore for weights in module:  model.layers.12.self_attn.o_proj
enable GaLore for weights in module:  model.layers.12.mlp.gate_proj
enable GaLore for weights in module:  model.layers.12.mlp.down_proj
enable GaLore for weights in module:  model.layers.12.mlp.up_proj
enable GaLore for weights in module:  model.layers.13.self_attn.q_proj
enable GaLore for weights in module:  model.layers.13.self_attn.k_proj
enable GaLore for weights in module:  model.layers.13.self_attn.v_proj
enable GaLore for weights in module:  model.layers.13.self_attn.o_proj
enable GaLore for weights in module:  model.layers.13.mlp.gate_proj
enable GaLore for weights in module:  model.layers.13.mlp.down_proj
enable GaLore for weights in module:  model.layers.13.mlp.up_proj
enable GaLore for weights in module:  model.layers.14.self_attn.q_proj
enable GaLore for weights in module:  model.layers.14.self_attn.k_proj
enable GaLore for weights in module:  model.layers.14.self_attn.v_proj
enable GaLore for weights in module:  model.layers.14.self_attn.o_proj
enable GaLore for weights in module:  model.layers.14.mlp.gate_proj
enable GaLore for weights in module:  model.layers.14.mlp.down_proj
enable GaLore for weights in module:  model.layers.14.mlp.up_proj
enable GaLore for weights in module:  model.layers.15.self_attn.q_proj
enable GaLore for weights in module:  model.layers.15.self_attn.k_proj
enable GaLore for weights in module:  model.layers.15.self_attn.v_proj
enable GaLore for weights in module:  model.layers.15.self_attn.o_proj
enable GaLore for weights in module:  model.layers.15.mlp.gate_proj
enable GaLore for weights in module:  model.layers.15.mlp.down_proj
enable GaLore for weights in module:  model.layers.15.mlp.up_proj
enable GaLore for weights in module:  model.layers.16.self_attn.q_proj
enable GaLore for weights in module:  model.layers.16.self_attn.k_proj
enable GaLore for weights in module:  model.layers.16.self_attn.v_proj
enable GaLore for weights in module:  model.layers.16.self_attn.o_proj
enable GaLore for weights in module:  model.layers.16.mlp.gate_proj
enable GaLore for weights in module:  model.layers.16.mlp.down_proj
enable GaLore for weights in module:  model.layers.16.mlp.up_proj
enable GaLore for weights in module:  model.layers.17.self_attn.q_proj
enable GaLore for weights in module:  model.layers.17.self_attn.k_proj
enable GaLore for weights in module:  model.layers.17.self_attn.v_proj
enable GaLore for weights in module:  model.layers.17.self_attn.o_proj
enable GaLore for weights in module:  model.layers.17.mlp.gate_proj
enable GaLore for weights in module:  model.layers.17.mlp.down_proj
enable GaLore for weights in module:  model.layers.17.mlp.up_proj
enable GaLore for weights in module:  model.layers.18.self_attn.q_proj
enable GaLore for weights in module:  model.layers.18.self_attn.k_proj
enable GaLore for weights in module:  model.layers.18.self_attn.v_proj
enable GaLore for weights in module:  model.layers.18.self_attn.o_proj
enable GaLore for weights in module:  model.layers.18.mlp.gate_proj
enable GaLore for weights in module:  model.layers.18.mlp.down_proj
enable GaLore for weights in module:  model.layers.18.mlp.up_proj
enable GaLore for weights in module:  model.layers.19.self_attn.q_proj
enable GaLore for weights in module:  model.layers.19.self_attn.k_proj
enable GaLore for weights in module:  model.layers.19.self_attn.v_proj
enable GaLore for weights in module:  model.layers.19.self_attn.o_proj
enable GaLore for weights in module:  model.layers.19.mlp.gate_proj
enable GaLore for weights in module:  model.layers.19.mlp.down_proj
enable GaLore for weights in module:  model.layers.19.mlp.up_proj
enable GaLore for weights in module:  model.layers.20.self_attn.q_proj
enable GaLore for weights in module:  model.layers.20.self_attn.k_proj
enable GaLore for weights in module:  model.layers.20.self_attn.v_proj
enable GaLore for weights in module:  model.layers.20.self_attn.o_proj
enable GaLore for weights in module:  model.layers.20.mlp.gate_proj
enable GaLore for weights in module:  model.layers.20.mlp.down_proj
enable GaLore for weights in module:  model.layers.20.mlp.up_proj
enable GaLore for weights in module:  model.layers.21.self_attn.q_proj
enable GaLore for weights in module:  model.layers.21.self_attn.k_proj
enable GaLore for weights in module:  model.layers.21.self_attn.v_proj
enable GaLore for weights in module:  model.layers.21.self_attn.o_proj
enable GaLore for weights in module:  model.layers.21.mlp.gate_proj
enable GaLore for weights in module:  model.layers.21.mlp.down_proj
enable GaLore for weights in module:  model.layers.21.mlp.up_proj
enable GaLore for weights in module:  model.layers.22.self_attn.q_proj
enable GaLore for weights in module:  model.layers.22.self_attn.k_proj
enable GaLore for weights in module:  model.layers.22.self_attn.v_proj
enable GaLore for weights in module:  model.layers.22.self_attn.o_proj
enable GaLore for weights in module:  model.layers.22.mlp.gate_proj
enable GaLore for weights in module:  model.layers.22.mlp.down_proj
enable GaLore for weights in module:  model.layers.22.mlp.up_proj
enable GaLore for weights in module:  model.layers.23.self_attn.q_proj
enable GaLore for weights in module:  model.layers.23.self_attn.k_proj
enable GaLore for weights in module:  model.layers.23.self_attn.v_proj
enable GaLore for weights in module:  model.layers.23.self_attn.o_proj
enable GaLore for weights in module:  model.layers.23.mlp.gate_proj
enable GaLore for weights in module:  model.layers.23.mlp.down_proj
enable GaLore for weights in module:  model.layers.23.mlp.up_proj
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(32000, 1024, padding_idx=31999)
    (layers): ModuleList(
      (0-23): 24 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (k_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (o_proj): Linear(in_features=1024, out_features=1024, bias=False)
          (rotary_emb): LlamaRotaryEmbedding()
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear(in_features=1024, out_features=2736, bias=False)
          (down_proj): Linear(in_features=2736, out_features=1024, bias=False)
          (up_proj): Linear(in_features=1024, out_features=2736, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm()
        (post_attention_layernorm): LlamaRMSNorm()
      )
    )
    (norm): LlamaRMSNorm()
  )
  (lm_head): Linear(in_features=1024, out_features=32000, bias=False)
)
[0m
[32m2026-02-21 11:46:29.173[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m307[0m - [1mTotal params: 367.97M[0m
[32m2026-02-21 11:46:29.176[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m308[0m - [1mTrainable params: 367.97M[0m
[32m2026-02-21 11:46:29.176[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m310[0m - [1mTotal params with GaLore enabled: 302.38M[0m
[32m2026-02-21 11:46:29.177[0m | [1mINFO    [0m | [36m__main__[0m:[36mmain[0m:[36m311[0m - [1mSaving model to /data/checkpoint/OSD_350M every 20000 update steps[0m
/workspace/Online-Subspace-Descent/galore_torch/adamw.py:48: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Update steps:   0%|                       | 96/60000 [03:05<28:26:28,  1.71s/it]Traceback (most recent call last):
  File "/workspace/Online-Subspace-Descent/torchrun_main.py", line 602, in <module>
    main(args)
  File "/workspace/Online-Subspace-Descent/torchrun_main.py", line 459, in main
    optimizer.step()
  File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/optim/optimizer.py", line 517, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Online-Subspace-Descent/galore_torch/adamw.py", line 96, in step
    grad = state["projector"].project(grad, state["step"], update_proj_stepsize_ratio = group["lr"]/self.init_lr, name = group["names"][i])
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/workspace/Online-Subspace-Descent/galore_torch/galore_projector.py", line 87, in project
    loss.backward()
  File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[rank0]: Traceback (most recent call last):
[rank0]:   File "/workspace/Online-Subspace-Descent/torchrun_main.py", line 602, in <module>
[rank0]:     main(args)
[rank0]:   File "/workspace/Online-Subspace-Descent/torchrun_main.py", line 459, in main
[rank0]:     optimizer.step()
[rank0]:   File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/optim/lr_scheduler.py", line 133, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/optim/optimizer.py", line 517, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/Online-Subspace-Descent/galore_torch/adamw.py", line 96, in step
[rank0]:     grad = state["projector"].project(grad, state["step"], update_proj_stepsize_ratio = group["lr"]/self.init_lr, name = group["names"][i])
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/workspace/Online-Subspace-Descent/galore_torch/galore_projector.py", line 87, in project
[rank0]:     loss.backward()
[rank0]:   File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/opt/conda/envs/galore/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: KeyboardInterrupt
